import gymnasium as gym
from gymnasium import spaces
import numpy as np

class GridWorldEnv(gym.Env):
    """
    ê°„ë‹¨í•œ 2D ê·¸ë¦¬ë“œ í™˜ê²½:
    - ì—ì´ì „íŠ¸ëŠ” (0, 0)ì—ì„œ ì‹œì‘í•˜ì—¬ (4, 4) ëª©í‘œì— ë„ë‹¬í•´ì•¼ í•¨
    - 4ê°œì˜ ë°©í–¥ìœ¼ë¡œ ì›€ì§ì¼ ìˆ˜ ìˆìŒ
    - ëª©í‘œ ë„ë‹¬ ì‹œ ë³´ìƒ +1, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ -0.1
    """
    def __init__(self):
        super().__init__()
        self.grid_size = 5
        self.action_space = spaces.Discrete(4)  # 0:ìƒ, 1:í•˜, 2:ì¢Œ, 3:ìš°
        self.observation_space = spaces.Box(
            low=0, high=self.grid_size - 1, shape=(2,), dtype=np.int32
        )

        self.agent_pos = None
        self.goal_pos = np.array([4, 4], dtype=np.int32)

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self.agent_pos = np.array([0, 0], dtype=np.int32)
        return self.agent_pos.copy(), {}

    def step(self, action):
        if action == 0:
            self.agent_pos[0] = max(self.agent_pos[0] - 1, 0)  # Up
        elif action == 1:
            self.agent_pos[0] = min(self.agent_pos[0] + 1, self.grid_size - 1)  # Down
        elif action == 2:
            self.agent_pos[1] = max(self.agent_pos[1] - 1, 0)  # Left
        elif action == 3:
            self.agent_pos[1] = min(self.agent_pos[1] + 1, self.grid_size - 1)  # Right

        terminated = np.array_equal(self.agent_pos, self.goal_pos)
        truncated = False  # ì‹œê°„ ì œí•œ ì¡°ê±´ ì—†ìŒ
        reward = 1.0 if terminated else -0.1

        observation = self.agent_pos.astype(np.int32).copy()
        return observation, reward, terminated, truncated, {}

    def render(self):
        grid = np.full((self.grid_size, self.grid_size), '-', dtype=str)
        grid[tuple(self.goal_pos)] = 'G'
        grid[tuple(self.agent_pos)] = 'A'
        print("\n".join(" ".join(row) for row in grid))
        print("------")


# from shimmy.gymnasium_compatibility import GymV26CompatibilityV0
from stable_baselines3 import PPO

# í™˜ê²½ ë˜í•‘
env = GridWorldEnv()
# wrapped_env = GymV26CompatibilityV0(env)

# ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
model = PPO("MlpPolicy", env, verbose=1)
model.learn(total_timesteps=10000)

# í•™ìŠµëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸
obs, _ = env.reset()  # âœ… íŠœí”Œ ì–¸íŒ©
done = False

while not done:
    action, _ = model.predict(obs)
    obs, reward, done, _, _ = env.step(action)  # âœ… Gymnasiumì€ 5ê°œì˜ return ê°’
    env.render()

    if done:
        print("ğŸ‰ ëª©í‘œì— ë„ë‹¬!")
        break

